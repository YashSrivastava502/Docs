#!/usr/bin/env python3
"""
main.py - Capacity Management Dashboard (single-file)

Features:
- Improved, modern UI using Dash + Bootstrap.
- KPIs (CPU cores, RAM GB, Storage TB), 30-day trends, filtered inventory.
- Events: manual form (toggle) and CSV upload (toggle). RAM entered in GB -> stored as MB.
- Robust inserts: attempts extended schema insert (pending_inventory/reconciled), falls back if columns absent.
- Immediate success feedback via Toasts.
- Embedded background reconciler (auto-mark pending events reconciled when inventory contains same server).
- Safe callback wiring (no mismatched inputs/outputs).
- Edit config.py before running (DB_CONFIG, TOTAL_CAPACITY, RESERVED_PERCENT).

Run:
  python main.py
"""

import os
import io
import base64
import threading
import time
from datetime import datetime, timedelta, timezone

import pandas as pd
import plotly.graph_objects as go
from sqlalchemy import create_engine, text

import logging
from logging.handlers import TimedRotatingFileHandler

from dash import Dash, dcc, html, Input, Output, State, callback_context, no_update
import dash_bootstrap_components as dbc

# Load your configuration (edit config.py)
from config import DB_CONFIG, TOTAL_CAPACITY, RESERVED_PERCENT

# ---------------- Logging ----------------
logger = logging.getLogger("capacity_dashboard")
logger.setLevel(logging.INFO)
handler = TimedRotatingFileHandler("capacity.log", when="midnight", backupCount=7)
handler.setFormatter(logging.Formatter("%(asctime)s - %(levelname)s - %(message)s"))
if not logger.handlers:
    logger.addHandler(handler)
else:
    # avoid duplicate handlers in some reload scenarios
    logger.handlers = [handler]

# ---------------- DB Engine ----------------
ENGINE = create_engine(
    f"postgresql+psycopg2://{DB_CONFIG['user']}:{DB_CONFIG['password']}@"
    f"{DB_CONFIG['host']}:{DB_CONFIG['port']}/{DB_CONFIG['dbname']}",
    pool_pre_ping=True
)

# ---------------- Filters / Constants ----------------
INVENTORY_STATUS = ("Running", "Running/Not in Production")
INVENTORY_LOCATION = "NJ Datacenter"
INVENTORY_TYPE = "Server/VM"

RECONCILE_INTERVAL = int(os.environ.get("RECONCILE_INTERVAL", "300"))
RUN_RECONCILER = os.environ.get("RUN_RECONCILER", "true").lower() in ("true", "1", "yes")

# ---------------- Helpers ----------------
def now_utc():
    return datetime.now(timezone.utc)

def fmt_cpu(v):
    try:
        return f"{int(round(float(v))):,} cores"
    except Exception:
        return "0 cores"

def fmt_ram_gb(v):
    try:
        return f"{float(v):,.2f} GB"
    except Exception:
        return "0.00 GB"

def fmt_storage_tb_from_gb(v):
    try:
        return f"{(float(v)/1024.0):,.2f} TB"
    except Exception:
        return "0.00 TB"

def parse_csv(contents, filename):
    """Parse uploaded CSV and return DataFrame with date, server, cpu, ram_gb, storage_gb."""
    if not contents:
        raise ValueError("No file uploaded")
    header, content_string = contents.split(",", 1)
    decoded = base64.b64decode(content_string)
    df = pd.read_csv(io.StringIO(decoded.decode("utf-8")))
    lc = [c.lower() for c in df.columns]
    required = {"date", "server", "cpu", "ram", "storage"}
    if not required.issubset(set(lc)):
        raise ValueError("CSV must contain headers: date,server,cpu,ram,storage")
    mapping = {orig: orig.lower() for orig in df.columns}
    df = df.rename(columns=mapping)
    df["date"] = pd.to_datetime(df["date"], errors="coerce")
    df = df.dropna(subset=["date"])
    df["server"] = df["server"].astype(str)
    for c in ["cpu", "ram", "storage"]:
        df[c] = pd.to_numeric(df[c], errors="coerce").fillna(0)
    df = df[["date", "server", "cpu", "ram", "storage"]].rename(columns={"ram": "ram_gb", "storage": "storage_gb"})
    return df

def detect_and_convert_ram_sum_to_gb(raw_sum, raw_max, baseline_gb):
    """Heuristic to adjust if ram_delta_gb values are actually MB."""
    try:
        rs = float(raw_sum); rm = float(raw_max)
    except Exception:
        return 0.0
    if baseline_gb <= 0:
        if abs(rm) > 1024 * 5:
            return rs / 1024.0
        return rs
    if abs(rm) > abs(baseline_gb) * 10 or abs(rm) > 1024 * 5:
        return rs / 1024.0
    return rs

# ---------------- Capacity & Trends ----------------
def calculate_capacity():
    """Compute KPIs (used, total, reserved, usable, available, pct)."""
    try:
        inv_sql = """
            SELECT
                COALESCE(SUM(NULLIF(TRIM(servercores), '')::numeric),0) AS cpu,
                COALESCE(SUM(NULLIF(TRIM(servermemory), '')::numeric)/1024.0,0) AS ram_gb,
                COALESCE(SUM(NULLIF(TRIM(totaldisk), '')::numeric),0) AS storage_gb
            FROM inventory
            WHERE assetstatus IN (:s1, :s2)
              AND assetlocation = :loc
              AND assettype = :atype
        """
        inv = pd.read_sql(text(inv_sql), ENGINE, params={"s1": INVENTORY_STATUS[0], "s2": INVENTORY_STATUS[1], "loc": INVENTORY_LOCATION, "atype": INVENTORY_TYPE})
        baseline_cpu = float(inv["cpu"].iloc[0])
        baseline_ram_gb = float(inv["ram_gb"].iloc[0])
        baseline_storage_gb = float(inv["storage_gb"].iloc[0])
    except Exception:
        logger.exception("Inventory read failed")
        baseline_cpu = baseline_ram_gb = baseline_storage_gb = 0.0

    try:
        ev_sql = """
            SELECT
                COALESCE(SUM(cpu_delta) FILTER (WHERE reconciled = false), 0) AS cpu_sum,
                COALESCE(SUM(ram_delta_gb) FILTER (WHERE reconciled = false), 0) AS ram_sum_raw,
                COALESCE(MAX(ABS(ram_delta_gb)) FILTER (WHERE reconciled = false), 0) AS ram_max_raw,
                COALESCE(SUM(storage_delta_gb) FILTER (WHERE reconciled = false), 0) AS storage_sum
            FROM capacity_events
        """
        ev = pd.read_sql(text(ev_sql), ENGINE)
        cpu_sum = float(ev["cpu_sum"].iloc[0])
        ram_sum_raw = float(ev["ram_sum_raw"].iloc[0])
        ram_max_raw = float(ev["ram_max_raw"].iloc[0])
        storage_sum = float(ev["storage_sum"].iloc[0])
    except Exception:
        logger.exception("Events aggregation failed")
        cpu_sum = ram_sum_raw = ram_max_raw = storage_sum = 0.0

    ram_sum_gb = detect_and_convert_ram_sum_to_gb(ram_sum_raw, ram_max_raw, baseline_ram_gb)

    used_cpu = baseline_cpu + cpu_sum
    used_ram_gb = baseline_ram_gb + ram_sum_gb
    used_storage_gb = baseline_storage_gb + storage_sum

    total_storage_gb = TOTAL_CAPACITY.get("STORAGE", 0) * 1024.0

    def mk(used, total):
        reserved = total * RESERVED_PERCENT
        usable = max(total - reserved, 0)
        used_val = max(used, 0)
        pct = (used_val / usable * 100) if usable > 0 else 0
        available = max(usable - used_val, 0)
        return {"used": used_val, "total": total, "reserved": reserved, "usable": usable, "available": available, "pct": pct}

    return {"CPU": mk(used_cpu, TOTAL_CAPACITY.get("CPU", 0)),
            "RAM": mk(used_ram_gb, TOTAL_CAPACITY.get("RAM", 0)),
            "STORAGE": mk(used_storage_gb, total_storage_gb)}

def build_trend(resource):
    """Return Plotly figure for last 30 days usage for resource."""
    try:
        inv_sql = """
            SELECT
                COALESCE(SUM(NULLIF(TRIM(servercores), '')::numeric),0) AS cpu,
                COALESCE(SUM(NULLIF(TRIM(servermemory), '')::numeric)/1024.0,0) AS ram_gb,
                COALESCE(SUM(NULLIF(TRIM(totaldisk), '')::numeric),0) AS storage_gb
            FROM inventory
            WHERE assetstatus IN (:s1, :s2) AND assetlocation = :loc AND assettype = :atype
        """
        inv = pd.read_sql(text(inv_sql), ENGINE, params={"s1": INVENTORY_STATUS[0], "s2": INVENTORY_STATUS[1], "loc": INVENTORY_LOCATION, "atype": INVENTORY_TYPE})
        baseline = float(inv["cpu" if resource == "CPU" else ("ram_gb" if resource == "RAM" else "storage_gb")].iloc[0])
    except Exception:
        baseline = 0.0

    today = datetime.today().date()
    start = today - timedelta(days=30)

    try:
        df = pd.read_sql(text("""
            SELECT DATE(event_time) AS d,
                   SUM(CASE WHEN :r='CPU' THEN COALESCE(cpu_delta,0)
                            WHEN :r='RAM' THEN COALESCE(ram_delta_gb,0)
                            WHEN :r='STORAGE' THEN COALESCE(storage_delta_gb,0) END) AS v
            FROM capacity_events
            WHERE event_time >= :start_date AND reconciled = false
            GROUP BY DATE(event_time) ORDER BY d
        """), ENGINE, params={"r": resource, "start_date": start})
    except Exception:
        df = pd.DataFrame()

    date_range = pd.date_range(start, today)
    if df.empty:
        df = pd.DataFrame({"d": date_range, "v": 0})
    else:
        df = df.set_index("d").reindex(date_range).fillna(0).reset_index()
        df.columns = ["d", "v"]

    if resource == "RAM":
        max_v = df["v"].abs().max()
        if max_v > (abs(baseline) * 10 if baseline > 0 else 1024 * 5):
            df["v"] = df["v"] / 1024.0

    df["used"] = baseline + df["v"].cumsum()

    if resource == "STORAGE":
        df["plot"] = df["used"] / 1024.0
        y_label = "Used (TB)"
    else:
        df["plot"] = df["used"]
        y_label = "Used (cores)" if resource == "CPU" else "Used (GB)"

    fig = go.Figure()
    fig.add_trace(go.Scatter(x=df["d"], y=df["plot"], mode="lines", line=dict(color="#0b5ed7", width=2), fill="tozeroy", name="Used"))
    fig.add_trace(go.Scatter(x=[df["d"].iloc[0], df["d"].iloc[-1]], y=[df["used"].iloc[0], df["used"].iloc[0]], mode="lines", line=dict(color="rgba(11,94,215,0.6)", dash="dot"), name="Baseline"))
    fig.update_layout(template="plotly_white", title=f"{resource} - Last 30 Days", xaxis_title="Date", yaxis_title=y_label, height=340)
    return fig

# ---------------- Auto-reconcile integrated ----------------
_reconcile_lock = threading.Lock()

def auto_reconcile_pending_events(thresholds=None, dry_run=False):
    if thresholds is None:
        thresholds = {"cpu_abs":1, "ram_mb_abs":512, "storage_gb_abs":5, "pct_tolerance":0.20}
    matched = []
    try:
        if not _reconcile_lock.acquire(blocking=False):
            logger.debug("Reconcile already running, skipping")
            return {"status":"skipped"}
        pending_df = pd.read_sql(text("""
            SELECT id, assetuniquename, cpu_delta, ram_delta_gb, storage_delta_gb
            FROM capacity_events
            WHERE pending_inventory = true AND reconciled = false
            ORDER BY event_time
        """), ENGINE)
        for _, row in pending_df.iterrows():
            sid = int(row["id"]); server = row["assetuniquename"]
            inv = pd.read_sql(text("SELECT servercores, servermemory, totaldisk FROM inventory WHERE assetuniquename = :s LIMIT 1"), ENGINE, params={"s": server})
            if inv.empty:
                continue
            try:
                inv_cpu = float(inv["servercores"].iloc[0]) if inv["servercores"].iloc[0] not in (None,"") else 0.0
            except:
                inv_cpu = 0.0
            try:
                inv_ram_mb = float(inv["servermemory"].iloc[0]) if inv["servermemory"].iloc[0] not in (None,"") else 0.0
            except:
                inv_ram_mb = 0.0
            try:
                inv_storage_gb = float(inv["totaldisk"].iloc[0]) if inv["totaldisk"].iloc[0] not in (None,"") else 0.0
            except:
                inv_storage_gb = 0.0

            ev_cpu = float(row["cpu_delta"] or 0)
            ev_ram_raw = float(row["ram_delta_gb"] or 0)
            ev_storage = float(row["storage_delta_gb"] or 0)
            ev_ram_mb = ev_ram_raw if abs(ev_ram_raw) > 1024 else ev_ram_raw * 1024.0

            cpu_ok = (abs(ev_cpu - inv_cpu) <= thresholds["cpu_abs"]) or (abs(ev_cpu - inv_cpu) <= thresholds["pct_tolerance"] * max(1, inv_cpu))
            ram_ok = (abs(ev_ram_mb - inv_ram_mb) <= thresholds["ram_mb_abs"]) or (abs(ev_ram_mb - inv_ram_mb) <= thresholds["pct_tolerance"] * max(1, inv_ram_mb))
            storage_ok = (abs(ev_storage - inv_storage_gb) <= thresholds["storage_gb_abs"]) or (abs(ev_storage - inv_storage_gb) <= thresholds["pct_tolerance"] * max(1, inv_storage_gb))

            if cpu_ok and ram_ok and storage_ok:
                if not dry_run:
                    with ENGINE.begin() as conn:
                        conn.execute(text("""
                            UPDATE capacity_events
                            SET reconciled = true, pending_inventory = false, reconciled_at = now(), reconciled_by = 'AUTO'
                            WHERE id = :id
                        """), {"id": sid})
                matched.append(sid)
        logger.info("Auto-reconcile matched %d events", len(matched))
        return {"matched_count": len(matched), "matched_ids": matched}
    except Exception as e:
        logger.exception("Auto-reconcile error")
        return {"error": str(e)}
    finally:
        try:
            _reconcile_lock.release()
        except:
            pass

def reconcile_loop(interval_seconds=RECONCILE_INTERVAL):
    time.sleep(2)
    while True:
        try:
            auto_reconcile_pending_events()
        except Exception:
            logger.exception("Background reconcile error")
        time.sleep(interval_seconds)

if RUN_RECONCILER:
    t = threading.Thread(target=reconcile_loop, daemon=True)
    t.start()
    logger.info("Embedded reconcile started (interval %s s)", RECONCILE_INTERVAL)

# ---------------- Robust insert helper ----------------
def insert_event_record(server, cpu_cores, ram_mb, storage_gb, source="MANUAL", event_time=None, pending=True):
    """Insert event; try extended schema first then fallback."""
    if event_time is None:
        event_time = now_utc()
    try:
        with ENGINE.begin() as conn:
            conn.execute(text("""
                INSERT INTO capacity_events
                (assetuniquename, cpu_delta, ram_delta_gb, storage_delta_gb, source, event_time, pending_inventory, reconciled)
                VALUES (:s, :c, :r, :st, :src, :et, :p, false)
            """), {"s": server, "c": cpu_cores, "r": ram_mb, "st": storage_gb, "src": source, "et": event_time, "p": pending})
        return True, "inserted"
    except Exception as e:
        logger.debug("Extended insert failed: %s", e)
    try:
        with ENGINE.begin() as conn:
            conn.execute(text("""
                INSERT INTO capacity_events
                (assetuniquename, cpu_delta, ram_delta_gb, storage_delta_gb, source, event_time)
                VALUES (:s, :c, :r, :st, :src, :et)
            """), {"s": server, "c": cpu_cores, "r": ram_mb, "st": storage_gb, "src": source, "et": event_time})
        return True, "inserted-fallback"
    except Exception as e:
        logger.exception("Fallback insert failed")
        return False, str(e)

# ---------------- App layout & callbacks ----------------
app = Dash(__name__, external_stylesheets=[dbc.themes.MINTY], suppress_callback_exceptions=True)
app.title = "Capacity Dashboard"

# Navbar + header
navbar = dbc.Navbar(
    dbc.Container([
        dbc.Row([
            dbc.Col(dbc.NavbarBrand("Capacity Dashboard", className="ms-2", style={"fontWeight": "700", "color":"#fff"})),
        ], align="center"),
        dbc.Row([
            dbc.Col(dbc.Button("Refresh", id="refresh", color="light"), width="auto")
        ], align="center"),
    ], fluid=True),
    color="#198754", dark=True, className="mb-3"
)

# Main layout
app.layout = dbc.Container([
    navbar,
    dbc.Row([
        dbc.Col(html.H2("Capacity Management", className="text-primary"), md=8),
        dbc.Col(html.Div(id="last-sync", className="text-end text-muted"), md=4)
    ], align="center"),
    dcc.Interval(id="interval", interval=3*60*1000, n_intervals=0),  # 3 minutes
    html.Hr(),

    dbc.Tabs([
        dbc.Tab(label="Overview", tab_id="overview", children=[
            dbc.Row(id="kpi-row", className="g-4"),
            html.Hr(),
            dbc.Row([
                dbc.Col(dbc.Card(dbc.CardBody([html.H6("CPU Trend"), dcc.Graph(id="cpu-trend")])), md=4),
                dbc.Col(dbc.Card(dbc.CardBody([html.H6("RAM Trend"), dcc.Graph(id="ram-trend")])), md=4),
                dbc.Col(dbc.Card(dbc.CardBody([html.H6("Storage Trend"), dcc.Graph(id="storage-trend")])), md=4),
            ], className="mb-3"),
            html.Div(id="alerts")
        ]),

        dbc.Tab(label="Events", tab_id="events", children=[
            dbc.Row([
                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader(html.H5("Add Event")),
                        dbc.CardBody([
                            dbc.Button("Show Manual Form", id="toggle-manual", color="primary", className="mb-2"),
                            dbc.Collapse(
                                dbc.Card(dbc.CardBody([
                                    dbc.Label("Server (assetuniquename)"),
                                    dbc.Input(id="server", placeholder="e.g. web-01", className="mb-2"),
                                    dbc.Row([
                                        dbc.Col(dbc.Input(id="cpu", type="number", placeholder="CPU Δ (cores)")),
                                        dbc.Col(dbc.Input(id="ram", type="number", placeholder="RAM Δ (GB)")),
                                    ], className="mb-2"),
                                    dbc.Label("Storage Δ (GB)"),
                                    dbc.Input(id="storage", type="number", placeholder="Storage Δ (GB)", className="mb-2"),
                                    dbc.Button("Submit Event", id="submit", color="success", className="w-100"),
                                    html.Div(id="msg", className="mt-2")
                                ])),
                                id="manual-collapse", is_open=False
                            ),
                            html.Hr(),
                            dbc.Button("Show CSV Uploader", id="toggle-upload", color="secondary", className="mb-2"),
                            dbc.Collapse(
                                dbc.Card(dbc.CardBody([
                                    html.P("CSV: date,server,cpu,ram,storage (ram in GB)"),
                                    dcc.Upload(id="csv-upload", children=dbc.Button("Select & Upload CSV", color="secondary", className="w-100")),
                                    html.Div(id="csv-msg", className="mt-2")
                                ])),
                                id="upload-collapse", is_open=False
                            )
                        ])
                    ])
                ], md=5),

                dbc.Col([
                    dbc.Card([
                        dbc.CardHeader(html.H5("Recent Events")),
                        dbc.CardBody([
                            dcc.Dropdown(id="recent-days", options=[{"label":"7 days","value":7},{"label":"30 days","value":30},{"label":"90 days","value":90}], value=30, clearable=False),
                            dbc.Button("Apply", id="recent-apply", color="info", className="mt-2 mb-2"),
                            html.Div(id="recent-events")
                        ])
                    ])
                ], md=7)
            ])
        ]),

        dbc.Tab(label="Inventory", tab_id="inventory", children=[
            html.Br(),
            dbc.Card(dbc.CardBody([html.H5("Filtered Inventory"), html.Div(id="inventory-table")])),
        ])
    ], active_tab="overview")
], fluid=True, style={"padding": "1rem 1.5rem"})

# Toggle callbacks
@app.callback(Output("manual-collapse", "is_open"), Input("toggle-manual", "n_clicks"), State("manual-collapse", "is_open"))
def toggle_manual(n, is_open):
    if n:
        return not is_open
    return is_open

@app.callback(Output("upload-collapse", "is_open"), Input("toggle-upload", "n_clicks"), State("upload-collapse", "is_open"))
def toggle_upload(n, is_open):
    if n:
        return not is_open
    return is_open

# Main callback - inputs match exactly the components in layout
@app.callback(
    [
        Output("kpi-row", "children"),
        Output("alerts", "children"),
        Output("cpu-trend", "figure"),
        Output("ram-trend", "figure"),
        Output("storage-trend", "figure"),
        Output("recent-events", "children"),
        Output("inventory-table", "children"),
        Output("msg", "children"),
        Output("csv-msg", "children"),
        Output("last-sync", "children")
    ],
    [
        Input("submit", "n_clicks"),
        Input("csv-upload", "contents"),
        Input("interval", "n_intervals"),
        Input("refresh", "n_clicks"),
        Input("recent-apply", "n_clicks")
    ],
    [
        State("server", "value"),
        State("cpu", "value"),
        State("ram", "value"),
        State("storage", "value"),
        State("csv-upload", "filename"),
        State("recent-days", "value")
    ],
    prevent_initial_call=False
)
def master_callback(submit_n, csv_contents, n_intervals, refresh_n, recent_apply_n,
                    server, cpu, ram, storage, filename, recent_days):
    """
    Handles:
      - Manual event submit
      - CSV upload
      - Periodic refresh (interval/refresh)
      - Recent events filter apply
    Returns KPI cards, alerts, trend figures, recent events table, inventory table,
    and toasts/alerts for manual & CSV actions plus last-sync timestamp.
    """
    ctx = callback_context
    triggered = ctx.triggered[0]["prop_id"].split(".")[0] if ctx.triggered else ""
    manual_msg = no_update
    csv_msg = no_update
    refresh_needed = False

    # Manual event submission
    if triggered == "submit" and submit_n:
        if not server or str(server).strip() == "":
            manual_msg = dbc.Alert("Server (assetuniquename) is required", color="danger")
        elif (cpu is None or float(cpu) == 0) and (ram is None or float(ram) == 0) and (storage is None or float(storage) == 0):
            manual_msg = dbc.Alert("Provide at least one non-zero delta (CPU, RAM, or Storage)", color="danger")
        else:
            try:
                # Check inventory existence quickly
                try:
                    r = ENGINE.execute(text("SELECT 1 FROM inventory WHERE assetuniquename = :s LIMIT 1"), {"s": server}).fetchone()
                    exists = r is not None
                except Exception:
                    df = pd.read_sql(text("SELECT 1 FROM inventory WHERE assetuniquename = :s LIMIT 1"), ENGINE, params={"s": server})
                    exists = not df.empty
                pending = not exists
                ram_mb = float(ram) * 1024.0 if ram not in (None, "") else 0.0
                success, info = insert_event_record(server, float(cpu or 0), ram_mb, float(storage or 0), source="MANUAL", pending=pending)
                if success:
                    manual_msg = dbc.Toast("Event submitted", header="Success", icon="success", duration=2500, is_open=True)
                    refresh_needed = True
                else:
                    manual_msg = dbc.Alert(f"Insert failed: {info}", color="danger")
            except Exception as e:
                logger.exception("Manual insert error")
                manual_msg = dbc.Alert(f"Error saving event: {e}", color="danger")

    # CSV upload handling
    if triggered == "csv-upload" and csv_contents:
        try:
            df = parse_csv(csv_contents, filename)
            inserted = 0
            errors = []
            for _, row in df.iterrows():
                server_name = row["server"]
                try:
                    r = ENGINE.execute(text("SELECT 1 FROM inventory WHERE assetuniquename = :s LIMIT 1"), {"s": server_name}).fetchone()
                    exists = r is not None
                except Exception:
                    temp = pd.read_sql(text("SELECT 1 FROM inventory WHERE assetuniquename = :s LIMIT 1"), ENGINE, params={"s": server_name})
                    exists = not temp.empty
                pending = not exists
                ram_mb = float(row["ram_gb"]) * 1024.0
                success, info = insert_event_record(server_name, float(row["cpu"]), ram_mb, float(row["storage_gb"]), source="CSV", event_time=row["date"], pending=pending)
                if success:
                    inserted += 1
                else:
                    errors.append({"server": server_name, "error": info})
            if inserted:
                csv_msg = dbc.Toast(f"Imported {inserted} rows", header="CSV Imported", icon="success", duration=3000, is_open=True)
                refresh_needed = True
            else:
                csv_msg = dbc.Alert(f"No rows imported. Errors: {errors}", color="danger")
        except Exception as e:
            logger.exception("CSV import error")
            csv_msg = dbc.Alert(f"CSV error: {e}", color="danger")

    # Refresh / compute when needed or on interval/refresh/recent-apply
    if refresh_needed or triggered in ("interval", "refresh", "") or triggered == "recent-apply":
        # KPIs
        cap = calculate_capacity()
        kpi_cards = []
        alerts = []
        for k in ("CPU", "RAM", "STORAGE"):
            d = cap[k]; pct = d.get("pct", 0)
            if pct > 100:
                color = "danger"
            elif pct > 90:
                color = "danger"
            elif pct > 80:
                color = "warning"
            else:
                color = "primary"

            if k == "CPU":
                used_text = fmt_cpu(d["used"]); total_text = fmt_cpu(d["total"]); reserved_text = fmt_cpu(d["reserved"]); usable_text = fmt_cpu(d["usable"]); available_text = fmt_cpu(d["available"])
            elif k == "RAM":
                used_text = fmt_ram_gb(d["used"]); total_text = fmt_ram_gb(d["total"]); reserved_text = fmt_ram_gb(d["reserved"]); usable_text = fmt_ram_gb(d["usable"]); available_text = fmt_ram_gb(d["available"])
            else:
                used_text = fmt_storage_tb_from_gb(d["used"]); total_text = fmt_storage_tb_from_gb(d["total"]); reserved_text = fmt_storage_tb_from_gb(d["reserved"]); usable_text = fmt_storage_tb_from_gb(d["usable"]); available_text = fmt_storage_tb_from_gb(d["available"])

            card = dbc.Card(dbc.CardBody([
                html.Div(html.H6(k), className="mb-1"),
                html.H4(used_text, className="mb-2"),
                html.Div([html.Small(f"Total: {total_text}", className="me-3"), html.Small(f"Reserved: {reserved_text}", className="me-3"), html.Small(f"Usable: {usable_text}")], className="d-flex flex-wrap mb-2"),
                dbc.Progress(value=min(pct,100), color=color, style={"height":"18px"}),
                html.Div(className="d-flex justify-content-between mt-2", children=[html.Small(f"Available: {available_text}"), html.Small(f"{pct:.1f}%")])
            ]), className="shadow-sm")
            kpi_cards.append(dbc.Col(card, md=4))
            if pct > 90:
                alerts.append(dbc.Alert(f"High {k} utilization ({pct:.1f}%)", color="warning", dismissable=True))

        # Trends
        cpu_fig = build_trend("CPU"); ram_fig = build_trend("RAM"); storage_fig = build_trend("STORAGE")

        # Recent events
        days = int(recent_days or 30)
        start_date = datetime.today().date() - timedelta(days=days)
        recent_df = pd.read_sql(text("""
            SELECT event_time, assetuniquename, cpu_delta, ram_delta_gb, storage_delta_gb
            FROM capacity_events
            WHERE DATE(event_time) >= :start_date
            ORDER BY event_time DESC LIMIT 200
        """), ENGINE, params={"start_date": start_date})

        if recent_df.empty:
            recent_table = html.P("No recent events", className="text-muted")
        else:
            def show_ram(v):
                try:
                    vv = float(v)
                except:
                    return v
                if abs(vv) > 1024 * 2:
                    return f"{vv/1024:.2f} GB"
                return f"{vv:.2f} GB"
            recent_df = recent_df.rename(columns={"event_time":"Event Time","assetuniquename":"Server","cpu_delta":"CPU Δ","storage_delta_gb":"Storage Δ (GB)"})
            recent_df["RAM Δ"] = recent_df["ram_delta_gb"].apply(show_ram)
            recent_table = dbc.Table.from_dataframe(recent_df[["Event Time","Server","CPU Δ","RAM Δ","Storage Δ (GB)"]], striped=True, hover=True, bordered=True, responsive=True)

        # Inventory (filtered)
        inv_df = pd.read_sql(text("""
            SELECT assetuniquename AS "Server Name", assetipaddress AS "IP Address", servercores AS "CPU (cores)",
                   ROUND(COALESCE(NULLIF(TRIM(servermemory), '')::numeric / 1024.0, 0), 2) AS "RAM (GB)",
                   totaldisk AS "Storage (GB)", assetstatus AS "Status"
            FROM inventory
            WHERE assetstatus IN (:s1, :s2) AND assetlocation = :loc AND assettype = :atype
            ORDER BY assetuniquename
        """), ENGINE, params={"s1": INVENTORY_STATUS[0], "s2": INVENTORY_STATUS[1], "loc": INVENTORY_LOCATION, "atype": INVENTORY_TYPE}).fillna("N/A")

        if inv_df.empty:
            inv_table = html.P("No inventory matching filters", className="text-muted")
        else:
            inv_table = dbc.Table.from_dataframe(inv_df[["Server Name","IP Address","CPU (cores)","RAM (GB)","Storage (GB)","Status"]], striped=True, hover=True, bordered=True, responsive=True)

        last_sync = f"Last update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        return kpi_cards, alerts, cpu_fig, ram_fig, storage_fig, recent_table, inv_table, manual_msg, csv_msg, last_sync

    # No updates: return no_update for all outputs to keep UI stable
    return no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update, no_update

# ---------------- Run ----------------
if __name__ == "__main__":
    # initial reconcile pass to reduce immediate duplicates
    if RUN_RECONCILER:
        try:
            auto_reconcile_pending_events()
        except Exception:
            logger.exception("Initial reconcile failed")
    app.run(host="0.0.0.0", port=8050, debug=False)
