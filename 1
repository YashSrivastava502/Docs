import paramiko
import pandas as pd
import psycopg2
import smtplib
from io import StringIO
from email.mime.text import MIMEText

# ========================
# SFTP CONFIG
# ========================
SFTP_HOST = "your_sftp_host"
SFTP_PORT = 22
SFTP_USER = "your_sftp_user"
SFTP_PASS = "your_sftp_password"

REMOTE_DIR = "/Network_devices_Backup/Global-Inventory"
REMOTE_FILE = "Global-SystemInventory.csv"

# ========================
# POSTGRES CONFIG
# ========================
PG_HOST = "10.7.32.181"
PG_PORT = 5432
PG_USER = "postgres"
PG_PASS = "mc6Qld8x0910"
TARGET_DB = "GlobalInventory"

# ========================
# SMTP CONFIG (NO AUTH)
# ========================
SMTP_SERVER = "your.smtp.server"
SMTP_PORT = 25
MAIL_FROM = "inventory-report@company.com"
MAIL_TO = ["ops@company.com"]

# ========================
# REQUIRED CSV COLUMNS
# ========================
CSV_COLUMNS = [
    "AssetUniqueName",
    "AssetIPAddress",
    "AssetStatus",
    "AssetLocation",
    "AssetType",
    "AssetsApplication/Owner",
    "CostCenter",
    "Environment",
    "ServerDomain",
    "ServerOS",
    "ServerCores",
    "ServerMemory",
    "TotalDisk",
    "ClusterName",
    "HostName"
]

DB_COLUMNS = [
    "assetuniquename",
    "assetipaddress",
    "assetstatus",
    "assetlocation",
    "assettype",
    "assetsapplicationowner",
    "costcenter",
    "environment",
    "serverdomain",
    "serveros",
    "servercores",
    "servermemory",
    "totaldisk",
    "clustername",
    "hostname"
]

# ========================
# FETCH CSV FROM SFTP
# ========================
def fetch_csv():
    transport = paramiko.Transport((SFTP_HOST, SFTP_PORT))
    transport.connect(username=SFTP_USER, password=SFTP_PASS)
    sftp = paramiko.SFTPClient.from_transport(transport)

    with sftp.open(f"{REMOTE_DIR}/{REMOTE_FILE}", "r") as f:
        data = f.read().decode("utf-8")

    sftp.close()
    transport.close()
    return data

# ========================
# ENSURE DATABASE EXISTS
# ========================
def ensure_database():
    conn = psycopg2.connect(
        host=PG_HOST, port=PG_PORT,
        user=PG_USER, password=PG_PASS,
        dbname="postgres"
    )
    conn.autocommit = True
    cur = conn.cursor()
    cur.execute("SELECT 1 FROM pg_database WHERE datname=%s", (TARGET_DB,))
    if not cur.fetchone():
        cur.execute(f'CREATE DATABASE "{TARGET_DB}"')
    cur.close()
    conn.close()

# ========================
# CONNECT TARGET DB
# ========================
def connect_db():
    return psycopg2.connect(
        host=PG_HOST, port=PG_PORT,
        user=PG_USER, password=PG_PASS,
        dbname=TARGET_DB
    )

# ========================
# CREATE TABLES
# ========================
def create_tables(conn):
    cur = conn.cursor()

    cur.execute("""
    CREATE TABLE IF NOT EXISTS inventory (
        assetuniquename TEXT PRIMARY KEY,
        assetipaddress TEXT,
        assetstatus TEXT,
        assetlocation TEXT,
        assettype TEXT,
        assetsapplicationowner TEXT,
        costcenter TEXT,
        environment TEXT,
        serverdomain TEXT,
        serveros TEXT,
        servercores TEXT,
        servermemory TEXT,
        totaldisk TEXT,
        clustername TEXT,
        hostname TEXT
    )
    """)

    cur.execute("""
    CREATE TABLE IF NOT EXISTS inventory_change_audit (
        id BIGSERIAL PRIMARY KEY,
        asset_unique_name TEXT,
        column_name TEXT,
        old_value TEXT,
        new_value TEXT,
        change_type TEXT,
        changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
    )
    """)

    conn.commit()
    cur.close()

# ========================
# AUTO-ALTER ALL COLUMNS TO TEXT (CASE-SAFE)
# ========================
def auto_alter_to_text(conn):
    cur = conn.cursor()
    cur.execute("""
        SELECT column_name
        FROM information_schema.columns
        WHERE table_name = 'inventory'
    """)
    cols = [row[0] for row in cur.fetchall()]

    for col in cols:
        cur.execute(f"""
            ALTER TABLE inventory
            ALTER COLUMN {col} TYPE TEXT
            USING {col}::TEXT
        """)

    conn.commit()
    cur.close()

# ========================
# FETCH EXISTING DATA
# ========================
def fetch_existing(conn):
    return pd.read_sql("SELECT * FROM inventory", conn)

# ========================
# UPSERT + AUDIT
# ========================
def upsert_and_audit(conn, df_new, df_old):
    cur = conn.cursor()
    changes = []

    if not df_old.empty:
        df_old = df_old.set_index("assetuniquename")

    for _, row in df_new.iterrows():
        asset = row["assetuniquename"]

        if not df_old.empty and asset in df_old.index:
            old_row = df_old.loc[asset]
            for col in DB_COLUMNS:
                old_val = str(old_row[col])
                new_val = str(row[col])
                if old_val != new_val:
                    cur.execute("""
                        INSERT INTO inventory_change_audit
                        (asset_unique_name, column_name, old_value, new_value, change_type)
                        VALUES (%s,%s,%s,%s,'UPDATE')
                    """, (asset, col, old_val, new_val))
                    changes.append(f"{asset} | {col} | {old_val} -> {new_val}")
        else:
            cur.execute("""
                INSERT INTO inventory_change_audit
                (asset_unique_name, column_name, old_value, new_value, change_type)
                VALUES (%s,'*',NULL,'NEW ROW','INSERT')
            """, (asset,))
            changes.append(f"{asset} | NEW ROW")

        cur.execute("""
        INSERT INTO inventory VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)
        ON CONFLICT (assetuniquename) DO UPDATE SET
        assetipaddress=EXCLUDED.assetipaddress,
        assetstatus=EXCLUDED.assetstatus,
        assetlocation=EXCLUDED.assetlocation,
        assettype=EXCLUDED.assettype,
        assetsapplicationowner=EXCLUDED.assetsapplicationowner,
        costcenter=EXCLUDED.costcenter,
        environment=EXCLUDED.environment,
        serverdomain=EXCLUDED.serverdomain,
        serveros=EXCLUDED.serveros,
        servercores=EXCLUDED.servercores,
        servermemory=EXCLUDED.servermemory,
        totaldisk=EXCLUDED.totaldisk,
        clustername=EXCLUDED.clustername,
        hostname=EXCLUDED.hostname
        """, tuple(row[col] for col in DB_COLUMNS))

    conn.commit()
    cur.close()
    return changes

# ========================
# SEND EMAIL
# ========================
def send_mail(changes):
    if not changes:
        return
    body = "Global Inventory Changes:\n\n" + "\n".join(changes)
    msg = MIMEText(body)
    msg["Subject"] = "Weekly Global Inventory Change Report"
    msg["From"] = MAIL_FROM
    msg["To"] = ", ".join(MAIL_TO)
    smtp = smtplib.SMTP(SMTP_SERVER, SMTP_PORT)
    smtp.sendmail(MAIL_FROM, MAIL_TO, msg.as_string())
    smtp.quit()

# ========================
# MAIN
# ========================
def main():
    csv_data = fetch_csv()

    df = pd.read_csv(StringIO(csv_data))[CSV_COLUMNS]
    df = df.rename(columns={"AssetsApplication/Owner": "AssetsApplicationOwner"})

    # Normalize column names to lowercase DB schema
    df.columns = [c.lower() for c in df.columns]

    # HARD NORMALIZATION
    df = df.fillna("")
    for col in df.columns:
        df[col] = df[col].astype(str).str.strip()

    ensure_database()
    conn = connect_db()
    create_tables(conn)
    auto_alter_to_text(conn)

    df_old = fetch_existing(conn)
    changes = upsert_and_audit(conn, df, df_old)

    send_mail(changes)
    conn.close()

    print("âœ… ETL completed successfully")

if __name__ == "__main__":
    main()
